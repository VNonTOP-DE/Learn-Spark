{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc5JKMfXx79cNcvzpqym+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VNonTOP-DE/Learn-Spark/blob/main/Learn_spark_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w4eI_WtwvDR",
        "outputId": "9ddc2905-53cf-43e4-fb5c-030e82839198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "PyykWCm7w-7P",
        "outputId": "815aa150-76ce-4b80-dd68-5226a03e612a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x792807cb51d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://06c8998d6d01:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>practice</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = spark.read.csv('walmart.csv',header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "RXUkM_hIxIeN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kbhwViJxIhG",
        "outputId": "31c76026-249e-43dd-ef19-908e0df79632"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+--------+------------+--------+--------------------+--------------------+-------------+----------------+--------------------+--------------------+-----+------------+\n",
            "|keyword|page|position|          id|    type|                name|               brand|averageRating|manufacturerName|    shortDescription|        thumbnailUrl|price|currencyUnit|\n",
            "+-------+----+--------+------------+--------+--------------------+--------------------+-------------+----------------+--------------------+--------------------+-----+------------+\n",
            "|perfume|   1|       1|1RQ2KVWAW4L5|Perfumes|Versace Bright Cr...|             Versace|          4.1|            NULL|Well known, Versa...|https://i5.walmar...|32.49|         USD|\n",
            "|perfume|   4|      41|671UM03HM2MS|Perfumes|Jimmy Choo Eau De...|          Jimmy Choo|          4.5|            NULL|jimmy choo has cr...|https://i5.walmar...|32.97|         USD|\n",
            "|perfume|   3|      37|4KRY0RE37RED|Perfumes|Emporio Armani St...|      Emporio Armani|          4.7|            NULL|Emporio Armani St...|https://i5.walmar...|87.95|         USD|\n",
            "|perfume|   3|      34|4ZAW52AOMED5|Perfumes|Yves Saint Lauren...|  Yves Saint Laurent|          4.7|            NULL|Yves Saint Lauren...|https://i5.walmar...| 54.6|         USD|\n",
            "|perfume|   3|      33|57DLMBVIMH85|Perfumes|Ralph Lauren Roma...|        Ralph Lauren|          4.7|            NULL|Ralph Lauren Roma...|https://i5.walmar...| 25.3|         USD|\n",
            "|perfume|   3|      32|2E6GYGZX3YB5|Perfumes|Clinique Happy He...|            Clinique|          4.6|            NULL|<p>Brand Clinique...|https://i5.walmar...|   20|         USD|\n",
            "|perfume|   3|      31|7H8II6SJKCNA|Perfumes|Viva La Juicy Ros...|       Juicy Couture|          4.3|            NULL|Viva La Juicy Ros...|https://i5.walmar...|33.31|         USD|\n",
            "|perfume|   3|      30|6VUZI9M7DYQV|Perfumes|BRIGHT CRYSTAL AB...|             Versace|          4.4|            NULL|<p>Versace Bright...|https://i5.walmar...|13.11|         USD|\n",
            "|perfume|   3|      29|3X3L5AF4RUWY|Perfumes|Jimmy Choo Eau De...|          Jimmy Choo|          4.5|            NULL|jimmy choo has cr...|https://i5.walmar...|43.49|         USD|\n",
            "|perfume|   3|      28|1FNYYPL1GS7M|Perfumes|Estee Lauder Beau...|        Estee Lauder|          4.4|            NULL|EAU DE PARFUM SPR...|https://i5.walmar...|   33|         USD|\n",
            "|perfume|   3|      27|689LYHN0FGKL|Perfumes|Charlie Blue by R...|             Charlie|          4.3|            NULL|Entice those arou...|https://i5.walmar...|13.98|         USD|\n",
            "|perfume|   3|      26|12Z2SWG2M1PA|Perfumes|5th Avenue by Eli...|     Elizabeth Arden|          4.6|            NULL|5th Avenue by Eli...|https://i5.walmar...| 24.4|         USD|\n",
            "|perfume|   3|      25|4JSLJQ9NOFKX|Perfumes|Tommy Hilfiger To...|      Tommy Hilfiger|          4.6|            NULL|Tommy Girl by Tom...|https://i5.walmar...|   32|         USD|\n",
            "|perfume|   3|      24|31PZKQ2C3WP5|Perfumes|Myself by Yves Sa...|  Yves Saint Laurent|          4.7|            NULL|Top notes are Cal...|https://i5.walmar...|76.43|         USD|\n",
            "|perfume|   3|      23|2560T0R9UGSD|Perfumes|Versace Eros Eau ...|             Versace|          4.5|            NULL|Versace Eros Eau ...|https://i5.walmar...|54.99|         USD|\n",
            "|perfume|   3|      22|6FD5RSZU6K06|Perfumes|Chanel Bleu De Ch...|              CHANEL|          3.7|            NULL|Chanel Bleu De Ch...|https://i5.walmar...|   78|         USD|\n",
            "|perfume|   3|      21|2HO82DD88JR7|Perfumes|Jimmy Choo Fever ...|          Jimmy Choo|          4.6|            NULL|Introduced in the...|https://i5.walmar...|41.99|         USD|\n",
            "|perfume|   3|      20|2TBHF6BEBTXI|Perfumes|Calvin Klein CK O...|        Calvin Klein|          4.6|            NULL|Calvin Klein Ck O...|https://i5.walmar...|35.87|         USD|\n",
            "|perfume|   3|      19|4W079OXL0LF5|Perfumes|Ralph Lauren Woma...|        Ralph Lauren|          4.9|            NULL|A scent that embo...|https://i5.walmar...|74.99|         USD|\n",
            "|perfume|   3|      18|6PLNTD0RU4RP|Perfumes|Ariana Grande God...|Ariana Grande God...|          4.4|            NULL|Empowered. Breath...|https://i5.walmar...|25.98|         USD|\n",
            "+-------+----+--------+------------+--------+--------------------+--------------------+-------------+----------------+--------------------+--------------------+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dyhz5ppxIkG",
        "outputId": "6b8eb5fc-2b52-466f-83af-df394ab17b59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- keyword: string (nullable = true)\n",
            " |-- page: string (nullable = true)\n",
            " |-- position: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- brand: string (nullable = true)\n",
            " |-- averageRating: string (nullable = true)\n",
            " |-- manufacturerName: string (nullable = true)\n",
            " |-- shortDescription: string (nullable = true)\n",
            " |-- thumbnailUrl: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- currencyUnit: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Cast 'averageRating' and 'price' to DoubleType\n",
        "training = training.withColumn(\"averageRating\", training[\"averageRating\"].cast(DoubleType()))\n",
        "training = training.withColumn(\"price\", training[\"price\"].cast(DoubleType()))\n",
        "training = training.withColumn(\"position\", training[\"position\"].cast(DoubleType()))"
      ],
      "metadata": {
        "id": "YGrGewyoxIsV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "featureassembler=VectorAssembler(inputCols=[\"averageRating\",\"price\"],outputCol=\"Independent Features\")"
      ],
      "metadata": {
        "id": "3feHyClwxImn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = featureassembler.transform(training)"
      ],
      "metadata": {
        "id": "KdlBfJwPxIpl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ok-pNtJLzVbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fywmNvzdxIvR",
        "outputId": "9ede9f45-d485-49b5-d1b8-79f4f6e628ac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+--------+------------+--------+--------------------+--------------------+-------------+----------------+--------------------+--------------------+-----+------------+--------------------+\n",
            "|keyword|page|position|          id|    type|                name|               brand|averageRating|manufacturerName|    shortDescription|        thumbnailUrl|price|currencyUnit|Independent Features|\n",
            "+-------+----+--------+------------+--------+--------------------+--------------------+-------------+----------------+--------------------+--------------------+-----+------------+--------------------+\n",
            "|perfume|   1|       1|1RQ2KVWAW4L5|Perfumes|Versace Bright Cr...|             Versace|          4.1|            NULL|Well known, Versa...|https://i5.walmar...|32.49|         USD|         [4.1,32.49]|\n",
            "|perfume|   4|      41|671UM03HM2MS|Perfumes|Jimmy Choo Eau De...|          Jimmy Choo|          4.5|            NULL|jimmy choo has cr...|https://i5.walmar...|32.97|         USD|         [4.5,32.97]|\n",
            "|perfume|   3|      37|4KRY0RE37RED|Perfumes|Emporio Armani St...|      Emporio Armani|          4.7|            NULL|Emporio Armani St...|https://i5.walmar...|87.95|         USD|         [4.7,87.95]|\n",
            "|perfume|   3|      34|4ZAW52AOMED5|Perfumes|Yves Saint Lauren...|  Yves Saint Laurent|          4.7|            NULL|Yves Saint Lauren...|https://i5.walmar...| 54.6|         USD|          [4.7,54.6]|\n",
            "|perfume|   3|      33|57DLMBVIMH85|Perfumes|Ralph Lauren Roma...|        Ralph Lauren|          4.7|            NULL|Ralph Lauren Roma...|https://i5.walmar...| 25.3|         USD|          [4.7,25.3]|\n",
            "|perfume|   3|      32|2E6GYGZX3YB5|Perfumes|Clinique Happy He...|            Clinique|          4.6|            NULL|<p>Brand Clinique...|https://i5.walmar...| 20.0|         USD|          [4.6,20.0]|\n",
            "|perfume|   3|      31|7H8II6SJKCNA|Perfumes|Viva La Juicy Ros...|       Juicy Couture|          4.3|            NULL|Viva La Juicy Ros...|https://i5.walmar...|33.31|         USD|         [4.3,33.31]|\n",
            "|perfume|   3|      30|6VUZI9M7DYQV|Perfumes|BRIGHT CRYSTAL AB...|             Versace|          4.4|            NULL|<p>Versace Bright...|https://i5.walmar...|13.11|         USD|         [4.4,13.11]|\n",
            "|perfume|   3|      29|3X3L5AF4RUWY|Perfumes|Jimmy Choo Eau De...|          Jimmy Choo|          4.5|            NULL|jimmy choo has cr...|https://i5.walmar...|43.49|         USD|         [4.5,43.49]|\n",
            "|perfume|   3|      28|1FNYYPL1GS7M|Perfumes|Estee Lauder Beau...|        Estee Lauder|          4.4|            NULL|EAU DE PARFUM SPR...|https://i5.walmar...| 33.0|         USD|          [4.4,33.0]|\n",
            "|perfume|   3|      27|689LYHN0FGKL|Perfumes|Charlie Blue by R...|             Charlie|          4.3|            NULL|Entice those arou...|https://i5.walmar...|13.98|         USD|         [4.3,13.98]|\n",
            "|perfume|   3|      26|12Z2SWG2M1PA|Perfumes|5th Avenue by Eli...|     Elizabeth Arden|          4.6|            NULL|5th Avenue by Eli...|https://i5.walmar...| 24.4|         USD|          [4.6,24.4]|\n",
            "|perfume|   3|      25|4JSLJQ9NOFKX|Perfumes|Tommy Hilfiger To...|      Tommy Hilfiger|          4.6|            NULL|Tommy Girl by Tom...|https://i5.walmar...| 32.0|         USD|          [4.6,32.0]|\n",
            "|perfume|   3|      24|31PZKQ2C3WP5|Perfumes|Myself by Yves Sa...|  Yves Saint Laurent|          4.7|            NULL|Top notes are Cal...|https://i5.walmar...|76.43|         USD|         [4.7,76.43]|\n",
            "|perfume|   3|      23|2560T0R9UGSD|Perfumes|Versace Eros Eau ...|             Versace|          4.5|            NULL|Versace Eros Eau ...|https://i5.walmar...|54.99|         USD|         [4.5,54.99]|\n",
            "|perfume|   3|      22|6FD5RSZU6K06|Perfumes|Chanel Bleu De Ch...|              CHANEL|          3.7|            NULL|Chanel Bleu De Ch...|https://i5.walmar...| 78.0|         USD|          [3.7,78.0]|\n",
            "|perfume|   3|      21|2HO82DD88JR7|Perfumes|Jimmy Choo Fever ...|          Jimmy Choo|          4.6|            NULL|Introduced in the...|https://i5.walmar...|41.99|         USD|         [4.6,41.99]|\n",
            "|perfume|   3|      20|2TBHF6BEBTXI|Perfumes|Calvin Klein CK O...|        Calvin Klein|          4.6|            NULL|Calvin Klein Ck O...|https://i5.walmar...|35.87|         USD|         [4.6,35.87]|\n",
            "|perfume|   3|      19|4W079OXL0LF5|Perfumes|Ralph Lauren Woma...|        Ralph Lauren|          4.9|            NULL|A scent that embo...|https://i5.walmar...|74.99|         USD|         [4.9,74.99]|\n",
            "|perfume|   3|      18|6PLNTD0RU4RP|Perfumes|Ariana Grande God...|Ariana Grande God...|          4.4|            NULL|Empowered. Breath...|https://i5.walmar...|25.98|         USD|         [4.4,25.98]|\n",
            "+-------+----+--------+------------+--------+--------------------+--------------------+-------------+----------------+--------------------+--------------------+-----+------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finalized_data = output.select(\"Independent Features\",\"position\")"
      ],
      "metadata": {
        "id": "zWGVSlzo1Glg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalized_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTGEqduw1Gn3",
        "outputId": "aeeead22-137e-4d22-8adb-1f0d617d61dd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+\n",
            "|Independent Features|position|\n",
            "+--------------------+--------+\n",
            "|         [4.1,32.49]|       1|\n",
            "|         [4.5,32.97]|      41|\n",
            "|         [4.7,87.95]|      37|\n",
            "|          [4.7,54.6]|      34|\n",
            "|          [4.7,25.3]|      33|\n",
            "|          [4.6,20.0]|      32|\n",
            "|         [4.3,33.31]|      31|\n",
            "|         [4.4,13.11]|      30|\n",
            "|         [4.5,43.49]|      29|\n",
            "|          [4.4,33.0]|      28|\n",
            "|         [4.3,13.98]|      27|\n",
            "|          [4.6,24.4]|      26|\n",
            "|          [4.6,32.0]|      25|\n",
            "|         [4.7,76.43]|      24|\n",
            "|         [4.5,54.99]|      23|\n",
            "|          [3.7,78.0]|      22|\n",
            "|         [4.6,41.99]|      21|\n",
            "|         [4.6,35.87]|      20|\n",
            "|         [4.9,74.99]|      19|\n",
            "|         [4.4,25.98]|      18|\n",
            "+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "training = training.filter(training[\"averageRating\"].isNotNull() & training[\"price\"].isNotNull())\n",
        "\n",
        "train_data,test_data = finalized_data.randomSplit([0.75,0.25])\n",
        "regressor = LinearRegression(featuresCol='Independent Features', labelCol='position')\n",
        "regressor = regressor.fit(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iXTPx0wX1Gqo",
        "outputId": "61110a22-9fb0-4ffa-bd08-57ff2c475f16"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o163.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (06c8998d6d01 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3146/0x000000084134e840`: (struct<averageRating:double,price:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3146/0x000000084134e840`: (struct<averageRating:double,price:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5567472c31a7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinalized_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Independent Features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'position'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o163.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (06c8998d6d01 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3146/0x000000084134e840`: (struct<averageRating:double,price:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda$3146/0x000000084134e840`: (struct<averageRating:double,price:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1264)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1265)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 34 more\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create a VectorAssembler with handleInvalid set to \"skip\"\n",
        "featureassembler = VectorAssembler(inputCols=[\"averageRating\", \"price\"], outputCol=\"Independent Features\", handleInvalid=\"skip\")\n",
        "\n",
        "# Transform the training data\n",
        "output = featureassembler.transform(training)\n",
        "\n",
        "# Select the features and target variable\n",
        "finalized_data = output.select(\"Independent Features\", \"position\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = finalized_data.randomSplit([0.75, 0.25])\n",
        "\n",
        "# Create and train the Linear Regression model\n",
        "regressor = LinearRegression(featuresCol='Independent Features', labelCol='position')\n",
        "regressor = regressor.fit(train_data)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hTK2QI-Szsc8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kVZkTqYWx6yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "training = training.filter(training[\"averageRating\"].isNotNull() & training[\"price\"].isNotNull())\n",
        "\n",
        "train_data,test_data = finalized_data.randomSplit([0.75,0.25])\n",
        "regressor = LinearRegression(featuresCol='Independent Features', labelCol='position')\n",
        "regressor = regressor.fit(train_data)"
      ],
      "metadata": {
        "id": "3VxEg6401GtZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.coefficients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vS-lId51GwE",
        "outputId": "5f7f16fc-ced4-4817-bddf-b1ab28bb0987"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([-5.2518, 0.0412])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.intercept"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfCyguO11Gy6",
        "outputId": "266d2b58-19ed-4f18-eeed-e248ee541b67"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44.02676368951738"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_results = regressor.evaluate(test_data)"
      ],
      "metadata": {
        "id": "lw4FI7eb1G17"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_results.predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkgiu_zL0DBd",
        "outputId": "3c56dda6-ba04-419c-fd7f-04ee296e9fe6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+------------------+\n",
            "|Independent Features|position|        prediction|\n",
            "+--------------------+--------+------------------+\n",
            "|          [3.0,98.0]|    37.0| 32.30833335325852|\n",
            "|         [3.3,64.92]|     1.0|29.370129455019022|\n",
            "|         [3.5,29.95]|     4.0| 26.87924909263721|\n",
            "|         [3.7,48.99]|    42.0|26.613208989065583|\n",
            "|          [3.7,76.9]|     6.0|27.762908889517213|\n",
            "|          [3.7,78.0]|    22.0| 27.80822131841355|\n",
            "|         [4.3,35.62]|     9.0|22.911385848439032|\n",
            "|         [4.3,39.87]|     8.0|23.086456596447608|\n",
            "|         [4.4,13.11]|    30.0|21.458950253426217|\n",
            "|         [4.4,31.21]|     6.0|22.204545674356865|\n",
            "|          [4.4,33.0]|    28.0|22.278281354106362|\n",
            "|         [4.4,54.95]|    26.0|23.182470276174193|\n",
            "|         [4.4,62.99]|    39.0|23.513662938289247|\n",
            "|          [4.5,7.98]|     2.0|20.722451034975542|\n",
            "|          [4.5,9.98]|     4.0| 20.80483726933252|\n",
            "|         [4.5,14.75]|    14.0|21.001328438273912|\n",
            "|         [4.5,18.27]|    16.0|21.146328210742194|\n",
            "|         [4.5,21.99]|    15.0|21.299566606646174|\n",
            "|         [4.5,23.86]|    12.0| 21.37659773576995|\n",
            "|         [4.5,34.99]|    20.0| 21.83507712996653|\n",
            "+--------------------+--------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HKZwtTO1G4W",
        "outputId": "c5014280-3305-4f94-a864-f0e6d9769a80"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11.021838615881787, 169.99350570118298)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}